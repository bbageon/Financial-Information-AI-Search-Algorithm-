# Financial-Information-AI-Search-Algorithm-
DACON AI Competition - 재정정보 AI 검색 알고리즘 경진대회

중앙정부 재정 정보에 대한 검색 기능을 개선하고 활용도를 높이는 질의 응답 알고리즘 개발

source pdf 마다 Vector DB 를 구축한 뒤 langchain 리이브러리와 llama-2-ko-7b 모델을 사용하여 RAG 프로세스를 통해 추론하는 과정을 담고 있다.

1. llama-2-ko-7b 모델은 7억개의 매개변수 => 굉장히 큰 모델 ( 사실상 학습이 불가능 )
2. PDF 마다 Vector DB 를 구축하여 모델을 통해 예측 ( 모델 자체는 성능이 뛰어난 편 )
3. Vector DB 를 한국어 질문에 맞게 잘 구축한다. (중요)
4. Langchain 라이브러리 조금 더 조사


- RAG 프로세스 
Retrieval-Augmented generation 개요
1. 질문 입력 
2. 문서 검색 -> 질문과 관련된 문서를 데이터베이스에서 검색, 다양한 검색 알고리즘 적용 가능 (중요)
3. 문서 추출 -> 검색된 문서에서 정보를 추출
4. 답변 생성 -> 모델(라마 모델)을 사용하여 답변 생성

PDF 문서 추출 과정
1. PyMuPDF 라이브러리 : PDF 에서 텍스트 추출
2. langchain.text_splitter 라이브러리 : 텍스트를 chunk 단위로 나눈다

* chunk?
- 긴 텍스트는 모델 입력으로 처리할 수 없음 -> 작게 분리하는 작업 : 청킹(chunking)
 => text_spliter 라이브러리는 텍스트 유형, 사용 사례에 맞춰 다양한 옵션 제공
    * 텍스트 분리 시, 고려할 점
     1. 텍스트가 어떻게 분리되는지
      - 각 청크가 독립적인 의미를 갖도록 함 ( 문장, 구절, 단락 등 문서 구조를 기준으로 나눌 수 있다.)
     2. 청크 크기가 어떻게 측정되는지
      - 청크 크기 조정( 단어 수, 문자 수 조절) : 모델의 입력 크기를 고려하여 결정

7월 31일 (수) , [Feat] 베이스코드 테스트 By 김건우
 모델 자체는 큰 편, PDF 문서 추출할 때, 청크 조절하고 Vector DB 어떻게 구축할 지 고려한다.
 결론적으로, 문서 검색 그리고 문서 추출 부분을 어떻게 할지 결정해야 함
 => PDF 파일 추출 시, 청크 크기를 문단 단위로 추출 해볼 것.